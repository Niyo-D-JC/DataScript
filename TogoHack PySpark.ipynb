{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ffe10c-5e58-41b4-a99d-ca1ae19b2a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f58068-b761-42ab-a26f-30e7945f11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession \n",
    "         .builder\n",
    "         # default url of the internally accessed Kubernetes API\n",
    "         # (This Jupyter notebook service is itself a Kubernetes Pod)\n",
    "         .master(\"k8s://https://kubernetes.default.svc:443\")\n",
    "         # Executors spark docker image: for simplicity reasons, this jupyter notebook is reused \n",
    "         .config(\"spark.kubernetes.container.image\", os.environ['IMAGE_NAME'])\n",
    "         # Name of the Kubernetes namespace\n",
    "         .config(\"spark.kubernetes.namespace\", os.environ['KUBERNETES_NAMESPACE'])\n",
    "         # Allocated memory to the JVM\n",
    "         # Stay careful, by default, the Kubernetes pods has a higher limit which depends on other parameters.\n",
    "         .config(\"spark.executor.memory\", \"4g\")\n",
    "         .config(\"spark.kubernetes.driver.pod.name\", os.environ['KUBERNETES_POD_NAME'])\n",
    "         # dynamic allocation configuration\n",
    "         .config(\"spark.dynamicAllocation.enabled\",\"true\")\n",
    "         .config(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n",
    "         .config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "         .config(\"spark.dynamicAllocation.maxExecutors\",\"5\")\n",
    "         # Ratio match the number of pods to create for a given number of parallel tasks \n",
    "         # (100 parallel, ratio of 1, one aims at 100 pods, with 0.5 it would be 50 pods)\n",
    "         .config(\"spark.dynamicAllocation.executorAllocationRatio\",\"1\")\n",
    "         .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",\"true\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9eb846c-55c1-4e60-a443-c0e8b7e3f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_path = \"s3a://juleschristian0/train.csv\"\n",
    "test_path = \"s3a://juleschristian0/test.csv\"\n",
    "\n",
    "train_df = spark.read.csv(train_path, header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(test_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b8e6d3-2fe9-4c02-8b1f-ed4619f4d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5bb17d-1d51-4be3-9b43-822cce8ebca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dcb355b-e2ff-4601-92cb-0a329b58df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [c for c in train_df.columns if str(train_df.schema[c].dataType) == \"DoubleType()\"]\n",
    "categorical_cols = [c for c in train_df.columns if str(train_df.schema[c].dataType) == \"StringType()\"]\n",
    "\n",
    "# Indexage et encodage des colonnes catégorielles\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\") for col in categorical_cols]\n",
    "\n",
    "# Assemblage des colonnes en un vecteur de caractéristiques\n",
    "assembler = VectorAssembler(inputCols=numeric_cols + [col+\"_vec\" for col in categorical_cols], outputCol=\"features\")\n",
    "\n",
    "# Standardisation des caractéristiques numériques\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5881c532-565b-4301-9b5b-53481958b6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4041"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numeric_cols) + len(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b21eb469-1cc8-4250-a114-baed8cdb0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"scaled_features\", seed=102)\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders + [assembler, scaler, rf])\n",
    "\n",
    "# Grille de paramètres pour la validation croisée\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# Validation croisée\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a475f3-f789-408e-969d-760acc48f9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===============>                                         (5 + 5) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:41:12.678 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 2 on 10.233.113.72: \n",
      "The executor with id 2 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/onyxia-jupyter-pyspark:py3.12.3-spark3.5.1\n",
      "\t container state: terminated\n",
      "\t container started at: 2024-06-22T17:29:02Z\n",
      "\t container finished at: 2024-06-22T17:41:10Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=========================>                               (8 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:41:18.879 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 5 on 10.233.113.91: \n",
      "The executor with id 5 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/onyxia-jupyter-pyspark:py3.12.3-spark3.5.1\n",
      "\t container state: terminated\n",
      "\t container started at: 2024-06-22T17:29:04Z\n",
      "\t container finished at: 2024-06-22T17:41:17Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:==================================>                     (11 + 4) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:41:26.003 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 3 on 10.233.115.21: \n",
      "The executor with id 3 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/onyxia-jupyter-pyspark:py3.12.3-spark3.5.1\n",
      "\t container state: terminated\n",
      "\t container started at: 2024-06-22T17:29:01Z\n",
      "\t container finished at: 2024-06-22T17:41:24Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n",
      "17:41:26.049 [dispatcher-CoarseGrainedScheduler] ERROR org.apache.spark.scheduler.TaskSchedulerImpl - Lost executor 4 on 10.233.115.13: \n",
      "The executor with id 4 exited with exit code 137(SIGKILL, possible container OOM).\n",
      "\n",
      "\n",
      "\n",
      "The API gave the following container statuses:\n",
      "\n",
      "\n",
      "\t container name: spark-kubernetes-executor\n",
      "\t container image: docker.io/inseefrlab/onyxia-jupyter-pyspark:py3.12.3-spark3.5.1\n",
      "\t container state: terminated\n",
      "\t container started at: 2024-06-22T17:29:02Z\n",
      "\t container finished at: 2024-06-22T17:41:24Z\n",
      "\t exit code: 137\n",
      "\t termination reason: OOMKilled\n",
      "      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=====================================>                  (12 + 5) / 18]\r"
     ]
    }
   ],
   "source": [
    "cv_model = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afe5e4-0bfd-440a-a730-3435e2c84203",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cv_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59256e-50a1-4044-9806-19a4e550315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation avec BinaryClassificationEvaluator pour calculer l'AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
